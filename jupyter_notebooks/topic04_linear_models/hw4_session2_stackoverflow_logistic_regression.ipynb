{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<img src=\"../../img/ods_stickers.jpg\">\n",
    "## Открытый курс по машинному обучению. Сессия № 2\n",
    "Авторы материала: Павел Нестеров. Материал распространяется на условиях лицензии [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/). Можно использовать в любых целях (редактировать, поправлять и брать за основу), кроме коммерческих, но с обязательным упоминанием автора материала."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Домашняя работа №4\n",
    "## <center> Логистическая регрессия в задаче тегирования вопросов StackOverflow\n",
    "\n",
    "**Надо вывести формулы, где это просится (да, ручка и бумажка), заполнить код в клетках и выбрать ответы в [веб-форме](https://docs.google.com/forms/d/1I_ticU8rpeoGJjsBUcaInpvgdxdq60hV7IcSvo4rlGo/).**\n",
    "\n",
    "## 0. Описание задачи\n",
    "\n",
    "В этой домашней работе мы с вами изучим и запрограммируем модель для прогнозирования тегов по тексту вопроса на базе многоклассовой логистической регрессии. В отличие от обычной постановки задачи классификации (multiclass), в данном случае один пример может принадлежать одновременно к нескольким классам (multilabel). Мы будем реализовывать онлайн версию алгоритма мультиклассовой классификации.\n",
    "\n",
    "Мы будем использовать небольшую выборку из протеггированных вопросов с сайта StackOverflow размером в 125 тысяч примеров (около 150 Мб, скачайте по [этой](https://drive.google.com/open?id=0B4bl7YMqDnViYVo0V2FubFVhMFE) ссылке).\n",
    "\n",
    "PS: Можно показать, что такая реализация совсем не эффективная и проще было бы использовать векторизированные вычисления. Для данного датасета так и есть. Но на самом деле подобные реализации используются в жизни, но естественно, написаны они не на Python. Например, в онлайн моделях прогнозирования [CTR](https://en.wikipedia.org/wiki/Click-through_rate) юзеру показывается баннер, затем в зависимости от наличия клика происходит обновление параметров модели. В реальной жизни параметров модели может быть несколько сотен миллионов, а у юзера из этих ста миллионов от силы сто или тысяча параметров отличны от нуля, векторизировать такие вычисления не очень эффективно. Обычно все это хранится в огромных кластерах в in-memory базах данных, а обработка пользователей происходит распределенно."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "plt.rcParams['figure.figsize'] = 16, 12\n",
    "from tqdm import tqdm_notebook\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# поменяйте на свой путь\n",
    "DS_FILE_NAME = '../../../data/stackoverflow_sample_125k.tsv'\n",
    "TAGS_FILE_NAME = '../../../data/top10_tags.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'c++', 'html', 'android', 'javascript', 'jquery', 'c#', 'ios', 'python', 'java', 'php'}\n"
     ]
    }
   ],
   "source": [
    "top_tags = []\n",
    "with open(TAGS_FILE_NAME, 'r') as f:\n",
    "    for line in f:\n",
    "        top_tags.append(line.strip())\n",
    "top_tags = set(top_tags)\n",
    "print(top_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Многоклассовая логистическая регрессия\n",
    "\n",
    "Вспомним, как получается логистическая регрессия для двух классов $\\left\\{0, 1\\right\\}$, вероятность принадлежности объекта к классу $1$ выписывается по теореме Байеса:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = 1 \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right) + p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} \\\\\n",
    "&=& \\dfrac{1}{1 + e^{-a}} \\\\\n",
    "&=& \\sigma\\left(a\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\vec{x}$ – вектор признаков объекта\n",
    "- $\\sigma$ – обозначение функции логистического сигмоида при скалярном аргументе\n",
    "- $a = \\log \\frac{p\\left(\\vec{x} \\mid c = 1\\right)p\\left(c = 1\\right)}{p\\left(\\vec{x} \\mid c = 0\\right)p\\left(c = 0\\right)} = \\sum_{i=0}^M w_i x^i$ – это отношение мы моделируем линейной функцией от признаков объекта и параметров модели\n",
    "\n",
    "Данное выражение легко обобщить до множества из $K$ классов, изменится только знаменатель в формуле Байеса. Запишем вероятность принадлежности объекта к классу $k$:\n",
    "$$\\large \\begin{array}{rcl}\n",
    "p\\left(c = k \\mid \\vec{x}\\right) &=& \\dfrac{p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right)}{\\sum_{i=1}^K p\\left(\\vec{x} \\mid c = i\\right)p\\left(c = i\\right)} \\\\\n",
    "&=& \\dfrac{e^{z_k}}{\\sum_{i=1}^{K}e^{z_i}} \\\\\n",
    "&=& \\sigma_k\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "где:\n",
    "- $\\sigma_k$ – обозначение функции softmax при векторном аргументе\n",
    "- $z_k = \\log p\\left(\\vec{x} \\mid c = k\\right)p\\left(c = k\\right) = \\sum_{i=0}^M w_{ki} x^i$ – это выражение моделируется линейной функций от признаков объекта и параметров класса $k$\n",
    "\n",
    "Для моделирования полного правдоподобия примера мы используем [категориальное распределение](https://en.wikipedia.org/wiki/Categorical_distribution), а лучше его логарифм (для удобства):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\mathcal{L} = \\log p\\left({\\vec{x}}\\right) &=& \\log \\prod_{i=1}^K \\sigma_i\\left(\\vec{z}\\right)^{y_i} \\\\\n",
    "&=& \\sum_{i=1}^K y_i \\log \\sigma_i\\left(\\vec{z}\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "Получается хорошо знакомая нам функция [cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (если домножить на $-1$). Правдоподобие нужно максимизировать, а, соответственно, перекрестную энтропию нужно минимизировать. Продифференцировав по параметрам модели, мы _легко_ получим правила обновления весов для градиентного спуска, **проделайте этот вывод, если вы его не делали** (если вы вдруг сдались, то на [этом](https://www.youtube.com/watch?v=-WiR16raQf4) видео есть разбор вывода, понимание этого вам понадобится для дальнейшего выполнения задания):\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} &=& x_m \\left(y_k - \\sigma_k\\left(\\vec{z}\\right)\\right)\n",
    "\\end{array}$$\n",
    "\n",
    "В стандартной формулировке получается, что вектор $\\left(\\sigma_1, \\sigma_2, \\ldots, \\sigma_K\\right)$ образует дискретное вероятностное распределение, т.е. $\\sum_{i=1}^K \\sigma_i = 1$. Но в нашей постановке задачи каждый пример может иметь несколько тегов или одновременно принадлежать к нескольким классам. Для этого мы немного изменим модель:\n",
    "- будем считать, что все теги независимы друг от друга, т.е. каждый исход – это логистическая регрессия на два класса (либо есть тег, либо его нет), тогда вероятность наличия тега у примера запишется следующим образом (каждый тег/класс как и в многоклассовой логрегрессии имеет свой набор параметров):\n",
    "$$\\large p\\left(\\text{tag}_k \\mid \\vec{x}\\right) = \\sigma\\left(z_k\\right) = \\sigma\\left(\\sum_{i=1}^M w_{ki} x^i \\right)$$\n",
    "- наличие каждого тега мы будем моделировать с помощью <a href=\"https://en.wikipedia.org/wiki/Bernoulli_distribution\">распределения Бернулли</a>\n",
    "\n",
    "Ваше первое задание –  записать упрощенное выражение логарифма правдоподобия примера с признаками $\\vec{x}$. Как правило, многие алгоритмы оптимизации имеют интерфейс для минимизации функции, мы последуем этой же традиции, и домножим полученное выражение на $-1$, а во второй части выведем формулы для минимизации полученного выражения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. $\\large -\\mathcal{L} = -\\sum_{i=1}^M y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "2. $\\large -\\mathcal{L} = -\\sum_{i=1}^K y_i \\log \\sigma\\left(z_i\\right) + \\left(1 - y_i\\right) \\log \\left(1 - \\sigma\\left(z_i\\right)\\right)$\n",
    "3. $\\large -\\mathcal{L} = -\\sum_{i=1}^K z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$\n",
    "4. $\\large -\\mathcal{L} = -\\sum_{i=1}^M z_i \\log \\sigma\\left(y_i\\right) + \\left(1 - z_i\\right) \\log \\left(1 - \\sigma\\left(y_i\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Вывод формулы обновления весов\n",
    "\n",
    "В качестве второго задания вам предоставляется возможность вывести формулу градиента для $-\\mathcal{L}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(\\sigma\\left(z_k\\right) - y_k\\right)$\n",
    "2. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = -x_m \\left(y_k - \\sigma\\left(z_k\\right)\\right)$\n",
    "3. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(\\sigma\\left(z_k\\right)x_m - y_k\\right)$\n",
    "4. $\\large -\\frac{\\partial \\mathcal{L}}{\\partial w_{km}} = \\left(y_k - \\sigma\\left(z_k\\right)x_m\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Имплементация базовой модели\n",
    "\n",
    "Вам предлагается каркас класса модели, разберите его внимательно, обращайте внимание на комментарии. Затем заполните пропуски, запустите полученную модель и ответьте на проверочный вопрос.\n",
    "\n",
    "Как вы могли уже заметить, при обновлении веса $w_{km}$ используется значение признака $x_m$, который равен $0$ если слова с индексом $m$ нет в предложении, и больше нуля, если такое слово есть. Соответственно, при вычислении линейной комбинации $z$ весов модели и признаков примера необходимо учитывать только ненулевые признаки объекта.\n",
    "\n",
    "Подсказка:\n",
    "- если реализовывать вычисление сигмоида так же, как в формуле, то при большом отрицательном значении $z$ вычисление $e^{-z}$ превратится в очень большое число, которое вылетит за допустимые пределы\n",
    "- в то же время $e^{-z}$ от большого положительного $z$ будет нулем\n",
    "- воспользуйтесь свойствами функции $\\sigma$ для того, чтобы пофиксить эту ошибку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = sigmoid(z)\n",
    "    \n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += -1 * y * np.log(sigma if sigma >= tolerance else tolerance) - (1 - y) * np.log((1 - sigma) if 1 - sigma >= tolerance else tolerance)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                        \n",
    "                self._loss.append(sample_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    t = 1 / (1 + np.exp(-1 * np.sign(x) * x))\n",
    "    return t if np.sign(x) >= 0 else 1 - t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "272ee4c868a54c6e866d06ef8e8d816f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# создадим эксемпляр модели и пройдемся по датасету\n",
    "model = LogRegressor()\n",
    "model.iterate_file()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим, действительно ли значение отрицательного логарифмического правдоподобия уменьшалось. Так как мы используем стохастический градентный спуск, не стоит ожидать плавного падения функции ошибки. Мы воспользуемся скользящим средним с окном в 10 000 примеров, чтобы хоть как то сгладить график."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA5kAAAKnCAYAAAAMUYlPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f8e67427f98>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of the loss function on the last 10k train samples: 19.76\n"
     ]
    }
   ],
   "source": [
    "print('Mean of the loss function on the last 10k train samples: %0.2f' % np.mean(model._loss[-35000:-25000]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вычислите среднее значение функции стоимости на последних 10 000 примеров тренировочного набора, к какому из значений ваш ответ ближе всего?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 17.54\n",
    "2. 18.64\n",
    "3. **19.74**\n",
    "4. 20.84"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 4. Тестирование модели\n",
    "\n",
    "В базовой модели первые 100 000 строк используются для обучения, а оставшиеся – для тестирования. Как вы можете заметить, значение отрицательного логарифмического правдоподобия не очень информативно, хоть и позволяет сравнивать разные модели. В качестве четвертого задания вам необходимо модифицировать базовую модель таким образом, чтобы метод `iterate_file` возвращал значение _точности_ на тестовой части набора данных. \n",
    "\n",
    "Точность определим следующим образом:\n",
    "- считаем, что тег у вопроса присутствует, если спрогнозированная вероятность тега больше 0.9\n",
    "- точность одного примера расчитывается как [коэффициент Жаккара](https://ru.wikipedia.org/wiki/Коэффициент_Жаккара) между множеством настоящих тегов и предсказанных моделью\n",
    "  - например, если у примера настоящие теги ['html', 'jquery'], а по версии модели ['ios', 'html', 'java'], то коэффициент Жаккара будет равен |['html', 'jquery'] $\\cap$ ['ios', 'html', 'java']| / |['html', 'jquery'] $\\cup$ ['ios', 'html', 'java']| = |['html']| / |['jquery', 'ios', 'html', 'java']| = 1/4\n",
    "- метод `iterate_file` возвращает **среднюю** точность на тестовом наборе данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LogRegressor():\n",
    "    \n",
    "    \"\"\"Конструктор\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    tags_top : list of string, default=top_tags\n",
    "        список тегов\n",
    "    \"\"\"\n",
    "    def __init__(self, tags=top_tags):      \n",
    "        # словарь который содержит мапинг слов предложений и тегов в индексы (для экономии памяти)\n",
    "        # пример: self._vocab['exception'] = 17 означает что у слова exception индекс равен 17\n",
    "        self._vocab = {}\n",
    "        \n",
    "        # параметры модели: веса\n",
    "        # для каждого класса/тега нам необходимо хранить собственный вектор весов\n",
    "        # по умолчанию у нас все веса будут равны нулю\n",
    "        # мы заранее не знаем сколько весов нам понадобится\n",
    "        # поэтому для каждого класса мы сосздаем словарь изменяемого размера со значением по умолчанию 0\n",
    "        # пример: self._w['java'][self._vocab['exception']]  содержит вес для слова exception тега java\n",
    "        self._w = dict([(t, defaultdict(int)) for t in tags])\n",
    "        \n",
    "        # параметры модели: смещения или вес w_0\n",
    "        self._b = dict([(t, 0) for t in tags])\n",
    "        \n",
    "        self._tags = set(tags)\n",
    "    \n",
    "    \"\"\"Один прогон по датасету\n",
    "    \n",
    "    Параметры\n",
    "    ----------\n",
    "    fname : string, default=DS_FILE_NAME\n",
    "        имя файла с данными\n",
    "        \n",
    "    top_n_train : int\n",
    "        первые top_n_train строк будут использоваться для обучения, остальные для тестирования\n",
    "        \n",
    "    total : int, default=10000000\n",
    "        информация о количестве строк в файле для вывода прогресс бара\n",
    "    \n",
    "    learning_rate : float, default=0.1\n",
    "        скорость обучения для градиентного спуска\n",
    "        \n",
    "    tolerance : float, default=1e-16\n",
    "        используем для ограничения значений аргумента логарифмов\n",
    "    \"\"\"\n",
    "    def iterate_file(self, \n",
    "                     fname=DS_FILE_NAME, \n",
    "                     top_n_train=100000, \n",
    "                     total=125000,\n",
    "                     learning_rate=0.1,\n",
    "                     tolerance=1e-16):\n",
    "        \n",
    "        self._loss = []\n",
    "        self._accuracy = []\n",
    "        n = 0\n",
    "        \n",
    "        # откроем файл\n",
    "        with open(fname, 'r') as f:            \n",
    "            # прогуляемся по строкам файла\n",
    "            for line in tqdm_notebook(f, total=total, mininterval=1):\n",
    "                pair = line.strip().split('\\t')\n",
    "                if len(pair) != 2:\n",
    "                    continue                \n",
    "                sentence, tags = pair\n",
    "                # слова вопроса, это как раз признаки x\n",
    "                sentence = sentence.split(' ')\n",
    "                # теги вопроса, это y\n",
    "                tags = set(tags.split(' '))\n",
    "                \n",
    "                # значение функции потерь для текущего примера\n",
    "                sample_loss = 0\n",
    "\n",
    "                # прокидываем градиенты для каждого тега\n",
    "                \n",
    "                predicted_tags = set()\n",
    "                \n",
    "                for tag in self._tags:\n",
    "                    # целевая переменная равна 1 если текущий тег есть у текущего примера\n",
    "                    y = int(tag in tags)\n",
    "                    \n",
    "                    # расчитываем значение линейной комбинации весов и признаков объекта\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    z = self._b[tag]\n",
    "   \n",
    "                    for word in sentence:\n",
    "                        # если в режиме тестирования появляется слово которого нет в словаре, то мы его игнорируем\n",
    "                        if n >= top_n_train and word not in self._vocab:\n",
    "                            continue\n",
    "                        if word not in self._vocab:\n",
    "                            self._vocab[word] = len(self._vocab)\n",
    "                        z += self._w[tag][self._vocab[word]]\n",
    "    \n",
    "                    # вычисляем вероятность наличия тега\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sigma = sigmoid(z)\n",
    "                    \n",
    "                    if n >= top_n_train and sigma > 0.9:\n",
    "                        predicted_tags.add(tag)\n",
    "                    \n",
    "                    # обновляем значение функции потерь для текущего примера\n",
    "                    # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                    sample_loss += -1 * y * np.log(sigma if sigma >= tolerance else tolerance) - (1 - y) * np.log((1 - sigma) if 1 - sigma >= tolerance else tolerance)\n",
    "                 \n",
    "                    \n",
    "                    # если мы все еще в тренировочной части, то обновим параметры\n",
    "                    if n < top_n_train:\n",
    "                        # вычисляем производную логарифмического правдоподобия по весу\n",
    "                        # ЗАПОЛНИТЕ ПРОПУСКИ В КОДЕ\n",
    "                        dLdw = y - sigma\n",
    "\n",
    "                        # делаем градиентный шаг\n",
    "                        # мы минимизируем отрицательное логарифмическое правдоподобие (второй знак минус)\n",
    "                        # поэтому мы идем в обратную сторону градиента для минимизации (первый знак минус)\n",
    "                        for word in sentence:                        \n",
    "                            self._w[tag][self._vocab[word]] -= -learning_rate*dLdw\n",
    "                        self._b[tag] -= -learning_rate*dLdw\n",
    "                    \n",
    "                n += 1\n",
    "                \n",
    "                if n >= top_n_train:\n",
    "                    jaccard = len(predicted_tags.intersection(tags)) / len(predicted_tags.union(tags))\n",
    "                    self._accuracy.append(jaccard)               \n",
    "                        \n",
    "                self._loss.append(sample_loss)\n",
    "        return np.mean(self._accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0acf9c6c4f6c4cbf9d7a726e444f1d0b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0.59\n"
     ]
    }
   ],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "# выведем полученное значение с точностью до двух знаков\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = set()\n",
    "a.add(5)\n",
    "a.add(5)\n",
    "a.add(3)\n",
    "len(a)\n",
    "b = []\n",
    "b.append(2)\n",
    "a = set([1,2,3])\n",
    "len(a.intersection(set([2,3])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.39\n",
    "2. 0.49\n",
    "3. 0.59\n",
    "4. 0.69"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 5. $L_2$-регуляризация\n",
    "\n",
    "В качестве пятого задания вам необходимо добавить в класс `LogRegressor` поддержку $L_2$-регуляризации. В методе `iterate_file` должен появиться параметр `lmbda=0.01` со значением по умолчанию. С учетом регуляризации новая функция стоимости примет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\frac{\\lambda}{2} \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2\n",
    "\\end{array}$$\n",
    "\n",
    "Градиент первого члена суммы мы уже вывели, а для второго он имеет вид:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "\\frac{\\partial}{\\partial w_{ki}} \\frac{\\lambda}{2} R\\left(W\\right) &=& \\lambda w_{ki}\n",
    "\\end{array}$$\n",
    "\n",
    "Если мы на каждом примере будем делать честное обновление всех весов, то все очень замедлится, ведь нам придется на каждой итерации пробегать по всем словам словаря. В ущерб теоретической точности вы используем грязный трюк: мы будем регуляризаровать только те слова, которые присутствуют в текущем предложении. Не забывайте, что смещение не регуляризируется. `sample_loss` тоже должен остаться без изменений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, к какому значению ближе всего полученное значение точности?\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. 0.3\n",
    "2. 0.35\n",
    "3. 0.4\n",
    "4. 0.52"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ElasticNet регуляризация, вывод\n",
    "Помимо $L_2$ регуляризации, часто используется $L_1$ регуляризация.\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\frac{\\lambda}{2} R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right|\n",
    "\\end{array}$$\n",
    "\n",
    "Если линейно объединить $L_1$ и $L_2$ регуляризацию, то полученный тип регуляризации называется ElasticNet:\n",
    "\n",
    "$$\\large \\begin{array}{rcl}\n",
    "L &=& -\\mathcal{L} + \\lambda R\\left(W\\right) \\\\\n",
    "&=& -\\mathcal{L} + \\lambda \\left(\\gamma \\sum_{k=1}^K\\sum_{i=1}^M w_{ki}^2 + \\left(1 - \\gamma\\right) \\sum_{k=1}^K\\sum_{i=1}^M \\left|w_{ki}\\right| \\right)\n",
    "\\end{array}$$\n",
    "- где $\\gamma \\in \\left[0, 1\\right]$\n",
    "\n",
    "В качестве шестого вопроса вам предлагается вывести формулу градиента ElasticNet регуляризации (не учитывая $-\\mathcal{L}$). \n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>:\n",
    "1. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) w_{ki}\\right)$ \n",
    "2. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma \\left|w_{ki}\\right| + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "3. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(2 \\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$\n",
    "4. $\\large \\frac{\\partial}{\\partial w_{ki}} \\lambda R\\left(W\\right) = \\lambda \\left(\\gamma w_{ki} + \\left(1 - \\gamma\\right) \\text{sign}\\left(w_{ki}\\right)\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. ElasticNet регуляризация, имплементация\n",
    "\n",
    "В качестве седьмой задачи, вам предлается изменить класс `LogRegressor` таким образом, чтобы метод `iterate_file` принимал два параметра со значениями по умолчанию `lmbda=0.0002` и `gamma=0.1`. Сделайте один проход по датасету с включенной ElasticNet регуляризацией и заданными значениями по умолчанию и ответьте на вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file()\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.59\n",
    "2. 0.69\n",
    "3. 0.79\n",
    "4. 0.82"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Самые важные слова для тега\n",
    "\n",
    "Прелесть линейных моделей в том, что они легко интерпретируемы. Вам предлагается вычислить, какие слова вносят наибольший вклад в вероятность каждого из тегов. А затем ответьте на контрольный вопрос."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для многих тегов наличие самого тега в предложении является важным сигналом, у многих сам тег является самым сильным сигналом, что неудивительно. Для каких из тегов само название тега не входит в топ-5 самых важных?\n",
    "\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. java, c#\n",
    "2. php, javascript\n",
    "3. html, jquery\n",
    "4. ios, android"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 9. Сокращаем размер словаря\n",
    "Сейчас количество слов в словаре около 90 000, если бы это была выборка из 10 миллионов вопросов с сайта StackOverflow, то размер словаря был бы миллионов 10. Регуляризировать модель можно не только изящно математически, но и топорно, например, ограничить размер словаря. Вам предоставляется возможность внести следующие изменения в класс `LogRegressor`:\n",
    "- добавить в метод `iterate_file` еще один аргумент со значением по умолчанию `update_vocab=True`\n",
    "- при `update_vocab=True` разрешать добавлять слова в словарь в режиме обучения\n",
    "- при `update_vocab=False` игнорировать слова не из словаря\n",
    "- добавить в класс метод `filter_vocab(n=10000)`, который оставит в словаре только топ-n самых популярных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# оставим только топ 10 000 слов\n",
    "model.filter_vocab(n=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# сделаем еще одну итерацию по датасету, уменьшив скорость обучения в 10 раз\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)\n",
    "plt.plot(pd.Series(model._loss[:-25000]).rolling(10000).mean());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Ответьте на вопрос,  к какому значению ближе всего полученное значение точности:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. 0.48\n",
    "2. 0.58\n",
    "3. 0.68\n",
    "4. 0.78"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Прогнозирование тегов для новых вопросов\n",
    "\n",
    "В завершение сегодняшней домашки, вам предлагается реализовать метод `predict_proba`, который принимает строку,  содержащую вопрос, а возвращает список предсказанных тегов вопроса с их вероятностями."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Обновите определение класса LogRegressor\n",
    "# Ваш код здесь"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = LogRegressor()\n",
    "acc = model.iterate_file(update_vocab=True)\n",
    "print('%0.2f' % acc)\n",
    "model.filter_vocab(n=10000)\n",
    "acc = model.iterate_file(update_vocab=False, learning_rate=0.01)\n",
    "print('%0.2f' % acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence = (\"I want to improve my coding skills, so I have planned write \" +\n",
    "            \"a Mobile Application.need to choose between Apple's iOS or Google's Android.\" +\n",
    "            \" my background: I have done basic programming in .Net,C/C++,Python and PHP \" +\n",
    "            \"in college, so got OOP concepts covered. about my skill level, I just know \" +\n",
    "            \"concepts and basic syntax. But can't write complex applications, if asked :(\" +\n",
    "            \" So decided to hone my skills, And I wanted to know which is easier to \" +\n",
    "            \"learn for a programming n00b. A) iOS which uses Objective C B) Android \" + \n",
    "            \"which uses Java. I want to decide based on difficulty level\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ответьте на вопрос, какой или какие теги ассоциируются с данным вопросом, если порог принятия равен $0.9$?:\n",
    "<font color=\"red\">Варианты ответа:</font>\n",
    "1. android\n",
    "2. ios\n",
    "3. ios, php\n",
    "4. c#, c++, ods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
